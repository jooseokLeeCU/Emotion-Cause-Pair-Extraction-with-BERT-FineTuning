{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3ebRoNol9_W"
   },
   "source": [
    "# Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "c0qDxOPBmCRu",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install --upgrade pip\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBk7GkNxHEPF"
   },
   "source": [
    "# About this project\n",
    "- This project was the final project of Natural Language Processing course (CSCI 5832) at University of Colorado Boulder\n",
    "- The main purpose of the final project was to solve a subset of one of [SemEval-2024 Tasks](https://semeval.github.io/SemEval2024/tasks.html).\n",
    "- This project is in particular focusing on the subtask 1 of [Task 3: The Competition of Multimodal Emotion Cause Analysis in Conversations](https://nustm.github.io/SemEval-2024_ECAC/).\n",
    "- The project team members are:\n",
    "  1. Jooseok Lee\n",
    "  2. Seungwook Lee\n",
    "- Simplified from the original project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwlhqedXIK6B"
   },
   "source": [
    "# Introduction\n",
    "- In this project, we aimed to solve the problem of textual Emotion-Cause Pair Extraction (ECPE), which is the first sub-task of SemEval-2024 Task 3,  using text classification and question answering framework.\n",
    "- The main purpose of textual ECPE is to find all sets of emotion-cause pairs where each utterance (i.e., small subset of a conversation) is matched with a single or multiple textual cause span(s) along with its emotional category.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/ECPE_overview.jpg\" alt=\"Overview of ECPE\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOdPDyOSNUic"
   },
   "source": [
    "# Approach\n",
    "- While the original paper solved this problem using a single solution, in this project we utilized two separate natural language processing (NLP) frameworks to solve it; text classification and question answering.\n",
    "- That is, we splitted the original problem into two separate sub problems and solved them independently.\n",
    "- In our approach, the text classification model is responsible for determining the emotional category of a given utterance.\n",
    "- A single utterance is given to a classification model and it predicts the six emotional categories of the utterance (i.e., Anger, Disgust, Fear, Joy, Sadness and Surprise).\n",
    "- We fine-tuned the publicly available text classification large language model (LLM) (i.e., BERT classification model) to solve this sub-problem.\n",
    "- Then, we utilized a question answering model to find textual cause span(s) of a given utterance.\n",
    "- In particular, we changed our data to Stanford Question Answering Dataset (SQuAD) format to fine-tune publicly available question answering LLM (i.e., DistilBERT question answering model).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Approach.jpg\" alt=\"Overview of ECPE\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JstE8OgtYtu"
   },
   "source": [
    "# Data Load\n",
    "Load the original json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqEfGbMErabk"
   },
   "outputs": [],
   "source": [
    "# Generalized code for handling file path\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/PersonalPage/ECPE-with-BERT')\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive (optional, if you need to access files there)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    # Define the data path (e.g., in a specific folder in Google Drive)\n",
    "    data_path = \"/content/drive/My Drive/PersonalPage/ECPE-with-BERT/data\"\n",
    "else:\n",
    "    # Define the local data path\n",
    "    data_path = \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0RwqDtgII8H"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.preprocess import process_json_file\n",
    "\n",
    "json_data = data_path + '/Subtask_1_train.json'\n",
    "\n",
    "# Replace 'conversation.json' with the path to your JSON file\n",
    "conversation_ids, conversations, emotion_labels = process_json_file(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMDihCRiTVVZ"
   },
   "source": [
    "# Text Classification\n",
    "- In this assignment, we focus on one aspect of text classification: sentiment analysis.\n",
    "- Sentiment analysis involves categorizing the emotional aspect of a given sentence or paragraph, identifying various emotional states such as positive, negative, or neutral.\n",
    "- This type of analysis plays a crucial role in understanding the emotions and attitudes of users from various text sources like customer feedback, online reviews, and social media posts.\n",
    "- Sentiment analysis has established itself as an important tool in understanding human emotions and attitudes through text data and is applied in various fields, including improving customer service, analyzing product reviews, public opinion research, and market analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM63NMfrTr4P"
   },
   "source": [
    "## Data Preprocessing\n",
    "- For sentiment analysis, it is essential to first classify the sentences and the emotions (labels) felt in those sentences.\n",
    "- Our data structure consists of conversations.\n",
    "- Each conversation includes several utterances and the emotions felt in those utterances.\n",
    "- The first step involves extracting the 'conversation' item from the JSON formatted data.\n",
    "- Subsequently, each utterance within the conversation and its corresponding emotional label are classified.\n",
    "- The classified emotional labels, which are a total of seven, are then converted into numbers for ease of model processing.\n",
    "- Following this, the process of tokenization of the utterances is carried out.\n",
    "- This step involves breaking down the text into smaller units known as tokens, which is crucial for transforming the text data into a format that can be processed by the model.\n",
    "- For this task, we use the ALBERT model, hence, we employ the Hugging Face Transformer library for optimized tokenization.\n",
    "- This library provides a tokenization method tailored to the ALBERT model, assisting the model in processing each utterance more accurately and effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_PGk3dNNVVn"
   },
   "outputs": [],
   "source": [
    "# Emotion to number mapping\n",
    "emotion_to_number = {\n",
    "    'joy': 0,\n",
    "    'sadness': 1,\n",
    "    'disgust': 2,\n",
    "    'fear': 3,\n",
    "    'anger': 4,\n",
    "    'neutral': 5,\n",
    "    'surprise': 6\n",
    "}\n",
    "\n",
    "# Function to map an emotion to a number\n",
    "def map_emotion_to_number(emotion):\n",
    "    return emotion_to_number.get(emotion, -1)  # Returns -1 if emotion is not found\n",
    "\n",
    "# Example usage\n",
    "emotion = 'joy'\n",
    "mapped_number = map_emotion_to_number(emotion)\n",
    "print(f\"The emotion '{emotion}' is mapped to number {mapped_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC5iABQBUfaP"
   },
   "outputs": [],
   "source": [
    "list_utterance = []\n",
    "list_emotion = []\n",
    "\n",
    "for emotions in emotion_labels:\n",
    "  for emotion in emotions:\n",
    "    list_emotion.append(emotion)\n",
    "\n",
    "for conversation in conversations:\n",
    "  for utterance in conversation:\n",
    "    list_utterance.append(utterance)\n",
    "\n",
    "print(len(list_emotion), ':', len(list_utterance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdIkJ5W9Urm2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine list_emotion and list_utterance\n",
    "data = list(zip(list_utterance, list_emotion))\n",
    "\n",
    "# Test set split\n",
    "train_val_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Validation set split\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Split emotions from utterances for model training\n",
    "train_utterances, train_emotions = zip(*train_data)\n",
    "val_utterances, val_emotions = zip(*val_data)\n",
    "test_utterances, test_emotions = zip(*test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2xDD7OMW2JP"
   },
   "source": [
    "# Model Training\n",
    "- In the process of model development, we adopted the fine-tuning approach using the pre-trained 'AlbertForSequenceClassification' model.\n",
    "- This method involves adapting an existing model to suit specific datasets and requirements.\n",
    "- A key change in the fine-tuning process was configuring the model to match the number of labels contained in our data. Since our dataset distinctly identifies seven emotional states, the model was set up to classify these seven categories.\n",
    "- By utilizing the 'AlbertForSequenceClassification', we were able to leverage the advantages of the ALBERT model, known for its efficiency and effectiveness in language classification tasks.\n",
    "- The ALBERT model employs the attention mechanism of the BERT model, which aids in understanding the nuanced use of words within sentences.\n",
    "- This is particularly effective in environments with limited computing resources, as ALBERT can deliver similar performance.\n",
    "- This is due to its smaller size, faster training speed, and lower memory usage.\n",
    "- The fine-tuning process demonstrates the flexibility and adaptability of pre-trained models in natural language processing, proving to be highly efficient for specific applications like ours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfRsfW-7Wz20"
   },
   "outputs": [],
   "source": [
    "# Imports libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel, AdamW\n",
    "from torch import nn\n",
    "\n",
    "from utils.model import SentimentDataBert\n",
    "from utils.model import SentimentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-0KjRmXarTG"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "dataset = SentimentDataBert(train_utterances, train_emotions)\n",
    "data_loader = dataset.get_data_loaders(batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPp3fDdxcM1H"
   },
   "outputs": [],
   "source": [
    "# Checkpoint saving function\n",
    "def save_checkpoint(model, optimizer, epoch, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG2M4vsLcShk"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model define\n",
    "model = SentimentClassifier(n_classes=len(list_emotion)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7P2emOFecfG9"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.01, correct_bias=False)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rvb84RxZeQok"
   },
   "outputs": [],
   "source": [
    "from utils.model import evaluate_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "val_dataset = SentimentDataBert(val_utterances, val_emotions)\n",
    "val_loader = val_dataset.get_data_loaders(batch_size=32, shuffle=True)\n",
    "\n",
    "predictions, true_labels = evaluate_model(model, val_loader, device='cuda')\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGJUGGmfi4t7"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ariPNlqIi3YQ"
   },
   "outputs": [],
   "source": [
    "test_dataset = SentimentDataBert(test_utterances, test_emotions)\n",
    "test_loader = test_dataset.get_data_loaders(batch_size=32, shuffle=True)\n",
    "\n",
    "predictions, true_labels = evaluate_model(model, test_loader, device='cuda')\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PP0QBhq_ouqQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNb5yfhBeP9YlRZ0y6CF9vF",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
